{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":53666,"databundleVersionId":6589269,"sourceType":"competition"},{"sourceId":7068401,"sourceType":"datasetVersion","datasetId":4070317}],"dockerImageVersionId":30587,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import pandas as pd \ndata=pd.read_csv('/kaggle/input/child-mind-institute-detect-sleep-states/sample_submission.csv')\ndata=pd.read_csv('/kaggle/input/child-mind-institute-detect-sleep-states/train_events.csv')\nprint(data.shape)\nprint(data.head(0))\nprint(data.nunique())\nprint(\"----------------------------------\")\nprint(data.dtypes)\nprint(\"----------------------------------\")\n#Encoding\nprint(data.dropna().shape)\nprint(data.isnull().sum)\nprint(\"------------------------------------\")\ndata_new=data.select_dtypes(exclude=[\"object\"])\nimport matplotlib.pyplot as plt\nx=[i for i in range(data.shape[0])]\nplt.plot(x)\n\nprint(data.duplicated())\nprint(data.duplicated().sum())\nprint(data.shape)\ndata=data.drop_duplicates()\nprint(data.shape)\n\n\nprint(\"_______________________________________\")\n\nimport numpy as np\nX = np.array([[2, 0, 1],\n[2, 1, 0],\n[1, 2, 0],\n[1, 0, 1],\n[2, 0, 2],\n[0, 1, 2]])\nY = np.array([4, 3, 7, 6, 11, 9])\nfrom sklearn.feature_selection import f_oneway\nX_best = f_oneway(X,Y)\nprint(\"Result = \", X_best[0])\nfrom sklearn.feature_selection import SelectKBest\nX_best = SelectKBest(f_oneway, k=2)\nX_best.fit_transform(X,Y)\nprint(\"index : \", X_best.get_support())\nX_new = X_best.fit_transform(X,Y)\nprint(\"X_new = \", X_new)\n\nimport pandas as pd\nmada=pd.DataFrame({'Subject-1': ['A','A','C','B','B','C','B'],'Subject-2': ['A','B','A','A','C','C','C'],'Subject-3': ['A','C','C','C','B','A','C'],'Acceptance': ['Yes','Yes','No','Yes','Yes','No','No']})\n\nfrom sklearn import preprocessing\nd_types=mada.dtypes\nfor i in range(mada.shape[1]):\n     if d_types[i]=='object':\n        Pr_data = preprocessing.LabelEncoder()\n        mada[mada.columns[i]]=Pr_data.fit_transform(mada[mada.columns[i]])\n        print(\"Column index-\", i, \": \", Pr_data.classes_)\n        print(mada)\nX=mada.iloc[:,:-1]\nY=mada.iloc[:,-1]\nfrom sklearn.feature_selection import chi2\nC= chi2(X, Y)\nprint(\"Values = \", C[0])\nfrom sklearn.feature_selection import SelectKBest\nXa = SelectKBest(chi2, k=2)\nXa.fit_transform(X,Y)\nprint(\"Index: \", Xa.get_support())\nX_new= Xa.fit_transform(X,Y)\nprint(\"X_new = \", X_new)\nfrom sklearn.datasets import load_iris\nfrom sklearn.feature_selection import mutual_info_regression\nX, Y = load_iris(return_X_y=True)\nValues = mutual_info_regression(X,Y)\nprint(\"Values = \", Values)\n\n\nprint(\"____________________________\")\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.datasets import load_iris\nfrom mlxtend.feature_selection import ExhaustiveFeatureSelector as EFS\niris = load_iris()\nX = iris.data\ny = iris.target\nknn = KNeighborsClassifier(n_neighbors=3)\nefs = EFS(knn, scoring='accuracy')\nefs = efs.fit(X, y)\nfeatures_index = efs.feature_groups\nprint(\"\\n\",features_index)\n\n\n\nfrom sklearn.svm import SVR\nfrom sklearn.feature_selection import RFE\nfrom sklearn import datasets\nX, Y = datasets.load_iris(return_X_y=True)\nestimator = SVR(kernel=\"linear\")\nselector = RFE(estimator, n_features_to_select=None)\nselector = selector.fit(X, Y)\nprint(selector.support_)\n\n\n\nfrom sklearn.linear_model import Lasso\nX = [[0.9], [1.4], [2.4], [1.2]]\nY = [0.8, 1.7, 2.1, 1.1]\nreg = Lasso(alpha=0.1)\nreg.fit(X,Y)\nprint(reg.coef_)\nprint(reg.intercept_)\n\n\nfrom sklearn.model_selection import train_test_split\nSpeed=[[1], [2], [3], [4], [5], [6], [7], [8], [9], [10]]\nX_train,X_test=train_test_split(Speed,test_size=0.3)\nprint(\"X-Train = \",X_train)\nprint(\"X-test = \",X_test)\n\nfrom sklearn.model_selection import train_test_split\nSpeed=[[1], [2], [3], [4], [5], [6], [7], [8], [9], [10]]\nX_train,X_test=train_test_split(Speed,test_size=0.3)\nprint(\"X-Train = \",X_train)\nprint(\"X-test = \",X_test)\n\n\nprint(\"_______________________\")\nprint(data.duplicated().sum())\nprint(data.shape)\ndata=data.drop_duplicates()\nprint(data.shape)\n\nfrom sklearn import preprocessing \npr_data = preprocessing.LabelEncoder()\n\nfrom sklearn import preprocessing\nX=[[5],\n   [6],\n   [12],\n   [2],\n   [8]]\nscaler =preprocessing.MinMaxScaler()\nXc = scaler.fit_transform(X)\nprint(Xc)\nscaler =preprocessing.RobustScaler()\nrobust_df=scaler.fit_transform(X)\nscaler =preprocessing.MaxAbsScaler()\nmaxabs_df=scaler.fit_transform(X)\nscaler =preprocessing.Normalizer()\nnormalizer_df=scaler.fit_transform(X)\nprint(\"_______________________________\")\n\nimport numpy as np\nimport pandas as pd\nfrom sklearn import preprocessing\ndata_set=pd.read_csv('/kaggle/input/child-mind-institute-detect-sleep-states/sample_submission.csv')\nd_types=data_set.dtypes\nfor i in range (data_set.shape[1]):\n    if d_types[i]=='object':\n        pr_data=preprocessing.LabelEncoder()\n        data_set[data_set.columns[i]]= pr_data.fit_transform(data_set[data_set.columns[i]])      \n\n        \nprint(\"__________________________\")  \n\n\nimport numpy as np\nSpeed = np.array([1, 2, 3, 4, 5])\nfrom sklearn.model_selection import LeavePOut\nmodel = LeavePOut(p=2)\niteration_number = 0\nfor index_train, index_test in model.split(Speed):\n    iteration_number = iteration_number + 1\n    print(\"iteration_number = \", iteration_number)\n    print(\"train data = \", Speed[index_train])\n    print(\"test data = \", Speed[index_test])\n    print(\"---------------------\")\nimport numpy as np\nSpeed = np.array([1, 2, 3, 4, 5])\nfrom sklearn.model_selection import LeavePOut\nmodel = LeavePOut(p=1)\niteration_number = 0\nfor index_train, index_test in model.split(Speed):\n    iteration_number = iteration_number + 1\n    print(\"iteration_number = \", iteration_number)\n    print(\"train data = \", Speed[index_train])\n    print(\"test data = \", Speed[index_test])\n    print(\"---------------------\")\nimport numpy as np\nSpeed = np.array([1, 2, 3, 4, 5])\nfrom sklearn.model_selection import KFold\nmodel = KFold(n_splits=5)\niteration_number = 0\nfor index_train, index_test in model.split(Speed):\n    iteration_number = iteration_number + 1\n    print(\"iteration_number = \", iteration_number)\n    print(\"train data = \", Speed[index_train])\n    print(\"test data = \", Speed[index_test])\n    print(\"---------------------\")\nimport numpy as np\nSpeed = np.array([1, 2, 3, 4, 5])\nfrom sklearn.model_selection import RepeatedKFold\nmodel = RepeatedKFold(n_splits=5, n_repeats=10)\niteration_number = 0\nfor index_train, index_test in model.split(Speed):\n    iteration_number = iteration_number + 1\n    print(\"iteration_number = \", iteration_number)\n    print(\"train data = \", Speed[index_train])\n    print(\"test data = \", Speed[index_test])\n    print(\"---------------------\")    \nY = [17, 15, 12, 13, 14, 18, 16]\nYpred = [19, 10, 21, 18, 11, 20, 14]\nimport numpy as np\nerror = sum(np.subtract(Ypred , Y))\nprint(\"Standard Error = \", error)\n\nfrom sklearn.metrics import mean_absolute_error\nMAE = mean_absolute_error(Y, Ypred)\nprint(\"Mean Absolute Error= \",MAE)\n\nfrom sklearn.metrics import mean_squared_error\nerrors = mean_squared_error(Y, Ypred)\nprint(\"mean squared error = \", errors)\n\n\nfrom sklearn.metrics import mean_squared_error\nRMSE = mean_squared_error(Y, Ypred, squared=False)\nprint(\"Root Mean Square Error= \",RMSE)\n\nfrom sklearn.metrics import mean_squared_log_error\nA = mean_squared_log_error(Ypred, Y)\nprint(\"MSLE = \" ,A)\n\n\nfrom sklearn.metrics import r2_score\nr2 = r2_score(Y,Ypred) \nn=len(Y)\np=1\nadj_r2_score = 1 - ((1-r2)*(n-1)/(n-p-1))\nprint(\"Adjusted R-Squared=\",adj_r2_score)\n\nX = [[2], [5], [6], [10], [12], [13], [16]]\nY = [1, 4, 6, 9, 11, 12, 14]\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, Y_train, Y_test = train_test_split(X,Y,test_size =0.3)\nprint(\"X-train = \", X_train)\nprint(\"X-test = \", X_test)\nprint(\"Y-train = \", Y_train)\nprint(\"Y-test = \", Y_test)\n\n\n\nfrom sklearn.linear_model import LinearRegression\nRegressionModel=LinearRegression()\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, Y_train, Y_test = train_test_split(X,Y,test_size =0.6)\nRegressionModel.fit(X_train,Y_train)\nb=RegressionModel.intercept_\na=RegressionModel.coef_\nAccuracy = RegressionModel.score(X_test,Y_test)\nprint(\"Accuracy = \", Accuracy)\n\nfrom sklearn.linear_model import LinearRegression\nRegressionModel=LinearRegression()\nRegressionModel.fit(X_train,Y_train)\nb=RegressionModel.intercept_\na=RegressionModel.coef_\nprint(\"a = \", a)\nprint(\"b = \", b)\n    \nAccuracy = RegressionModel.score(X_test,Y_test)\nprint(\"Accuracy = \", Accuracy)\n\nfrom sklearn.metrics import hamming_loss\ny_true = [0, 0, 1, 0, 1]\ny_pred = [1, 0, 0, 0, 0]\nA = hamming_loss(y_true, y_pred)\nprint(\"Hamming Loss = \", A)\n\nfrom sklearn.metrics import zero_one_loss\ny_true = [0, 0, 1, 0, 1]\ny_pred = [1, 0, 0, 0, 0]\nA = zero_one_loss(y_true, y_pred)\nprint(\"Zero-one Loss (Average) = \" ,A)\nB = zero_one_loss(y_true, y_pred, normalize=False)\nprint(\"Zero-one Loss (Count) = \" ,B)\nfrom sklearn.metrics import confusion_matrix\ny_true = [0, 1, 1, 0, 1, 1]\ny_pred = [1, 1, 0, 0, 0, 1]\nA = confusion_matrix(y_true, y_pred)\nprint(A)\n\nTN, FP, FN, TP = confusion_matrix(y_true, y_pred).ravel()\nprint('TN = ', TN)\nprint('FP = ', FP)\nprint('FN = ', FN)\nprint('TP = ', TP)\n\nfrom sklearn import metrics\ny_true = [0, 0, 1, 0, 1, 1, 0, 0, 1]\ny_pred = [1, 0, 1, 0, 1, 0, 0, 1, 0]\nA = metrics.precision_score(y_true, y_pred)\nprint(\"Precision score = \",A)\n\nB = metrics.recall_score(y_true, y_pred)\nprint(\"Recall score = \",B)\n\nC = metrics.f1_score(y_true, y_pred)\nprint(\"F1 score = \",C)\nD = metrics.fbeta_score(y_true, y_pred, beta=0.5)\nprint(\"Fbeta score (beta = 0.5) = \",D)\nE = metrics.fbeta_score(y_true, y_pred, beta=1)\nprint(\"Fbeta score (beta = 1) = \",E)\nF = metrics.fbeta_score(y_true, y_pred, beta=2)\nprint(\"Fbeta score (beta = 2) = \",F)\n\n\nfrom sklearn.metrics import classification_report\ny_true = [0, 0, 1, 0, 1, 1, 0, 0, 1]\ny_pred = [1, 0, 1, 0, 1, 0, 0, 1, 0]\nClasses = ['class 0', 'class 1']\nprint(classification_report(y_true, y_pred, target_names = Classes))\n\nfrom sklearn.metrics import multilabel_confusion_matrix\ny_true = [[0, 1, 1, 0, 0, 0, 1, 1, 0],\n[1, 1, 1, 1, 0, 1, 0, 0, 1]]\ny_pred = [[1, 1, 1, 0, 1, 0, 0, 0, 1],\n[0, 0, 1, 0, 1, 1, 0, 1, 1]]\nA = multilabel_confusion_matrix(y_true, y_pred)\nprint(A)\n\nX = [[2], [3], [4], [5], [6], [8], [10], [12], [13], [15]]\nY = [0, 1, 0, 0, 1, 1, 1, 0, 1, 1]\n\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, Y_train, Y_test = train_test_split(X,Y,test_size =0.3)\n\nprint(\"X-train = \", X_train)\nprint(\"X-test = \", X_test)\nprint(\"Y-train = \", Y_train)\nprint(\"Y-test = \", Y_test)\n                                                    \n                       \nfrom sklearn.linear_model import LogisticRegression\nmodel=LogisticRegression()\n\nmodel.fit(X_train,Y_train)\n\nb= model.intercept_\na= model.coef_\n\nprint(\"a = \", a)\nprint(\"b = \", b)\n\nY_pred = model.predict(X_test)\nprint(\"Y_pred = \", Y_pred)","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-11-27T19:49:39.208769Z","iopub.execute_input":"2023-11-27T19:49:39.209192Z","iopub.status.idle":"2023-11-27T19:49:40.941263Z","shell.execute_reply.started":"2023-11-27T19:49:39.209157Z","shell.execute_reply":"2023-11-27T19:49:40.940076Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}